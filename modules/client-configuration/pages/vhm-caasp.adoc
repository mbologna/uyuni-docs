[[vhm-caasp]]
= VHM and SUSE CaaS Platform

You can use a {productname} VHM to gather instances from {caasp}.

The VHM allows {productname} to obtain and report information about your virtual machines.
For more information on VHMs, see xref:client-configuration:vhm.adoc[].



== Onboarding CaaSP nodes

You can register each {caasp} node to {productname} using the same method as you would a Salt client.
For more information, see xref:client-configuration:registration-overview.adoc[].

We recommend that you create an activation key to associate {caasp} channels, and to onboard the associated nodes.
For more on activation keys, see xref:client-configuration:clients-and-activation-keys.adoc[].

If you are using ``cloud-init``, we recommended that you use a bootstrap script in the ``cloud-init`` configuration.
For more on bootstrapping, see xref:client-configuration:registration-bootstrap.adoc[].

When you have added the {caasp} nodes to {productname}, the registered system will automatically apply the system lock Salt formula to prevent unintended actions on the client.
When a system is locked, the {webui} shows a warning and you can schedule actions using the {webui} or the API, but the action will fail.
For more information about system locks, see xref:client-configuration:system-locking.adoc[].

You can disable the System Lock formula from being automatically applied by editing the configuration file.
Open [path]``/etc/rhn/rhn.conf`` and add this line at the end of the file:

Add this line at the end of the [path]``/etc/rhn/rhn.conf`` file:

----
java.automatic_system_lock_cluster_nodes = false
----

Restart the spacewalk service to pick up the changes:

----
spacewalk-service restart
----

Updates related to {k8s} are managed using the ``skuba-update`` tool.
For more information, see https://documentation.suse.com/suse-caasp/4/html/caasp-admin.


[WARNING]
====
When using Salt or {productname} (either via UI or API) on any {caasp} nodes:

* Do not apply a patch (if the patch is marked as interactive)
* Do not mark a system to automatically install patches
* Do not perform an SP migration
* Do not reboot a node
* Do not issue any power management action via Cobbler
* Do not install a package if it breaks or conflicts the `patterns-caasp-Node-x.y`
* Do not remove a package if it breaks or conflicts or is one of the packages related with the `patterns-caasp-Node-x.y`
* Do not upgrade a package if it breaks or conflicts or is one of the packages related with the `patterns-caasp-Node-x.y`

Issuing those operations could render your {caasp} cluster unusable.
{productname} will not stop you from issuing these operations if the system is not locked.
====

== Autoinstallation profile of a {caasp}{nbsp}4 node

{caasp}{nbsp}4 provides an AutoYaST profile to autoinstall a node in the `patterns-caasp-Management` package (see https://documentation.suse.com/suse-caasp/4.2/single-html/caasp-deployment/#_autoyast_preparation).
The following is the {caasp}{nbsp}4 template customized to make use of {productname} channels and minion registration script that you can use as a starting point to automatically install a {caasp}{nbsp}4 using the Autoinstallation feature of {productname}.

.Example: Script for autoinstallation of a {caasp}{nbsp}4 node
----
<?xml version="1.0"?>
<!DOCTYPE profile>
<profile xmlns="http://www.suse.com/1.0/yast2ns" xmlns:config="http://www.suse.com/1.0/configns">
  <bootloader>
    <global>
      <generic_mbr>true</generic_mbr>
      <gfxmode>auto</gfxmode>
      <hiddenmenu>false</hiddenmenu>
      <os_prober>false</os_prober>
      <terminal>gfxterm</terminal>
      <timeout config:type="integer">8</timeout>
      <suse_btrfs config:type="boolean">true</suse_btrfs>
    </global>
  </bootloader>
  <general>
    <ask-list config:type="list"/>
    <mode>
      <confirm config:type="boolean">false</confirm>
    </mode>
    <proposals config:type="list"/>
    <storage>
      <partition_alignment config:type="symbol">align_optimal</partition_alignment>
      <start_multipath config:type="boolean">false</start_multipath>
    </storage>
    <signature-handling>
      <accept_file_without_checksum config:type="boolean">true</accept_file_without_checksum>
      <accept_non_trusted_gpg_key config:type="boolean">true</accept_non_trusted_gpg_key>
      <accept_unknown_gpg_key config:type="boolean">true</accept_unknown_gpg_key>
      <accept_unsigned_file config:type="boolean">true</accept_unsigned_file>
      <accept_verification_failed config:type="boolean">false</accept_verification_failed>
      <import_gpg_key config:type="boolean">true</import_gpg_key>
    </signature-handling>
  </general>

  <!-- configure local storage -->
 <partitioning config:type="list">
    <drive>
      <use>all</use>
      <partitions config:type="list">
        <partition>
          <mount>/boot/efi</mount>
          <size>200mb</size>
          <partition_id config:type="integer">1</partition_id>
          <filesystem config:type="symbol">vfat</filesystem>
        </partition>
        <partition>
          <filesystem config:type="symbol">ext4</filesystem>
          <mount>/</mount>
          <size>max</size>
        </partition>
      </partitions>
    </drive>
  </partitioning>

  <!-- don't import any ssh configuration from previously-installed OS -->
  <ssh_import>
    <copy_config config:type="boolean">false</copy_config>
    <import config:type="boolean">false</import>
  </ssh_import>

  <!-- configure language and timezone -->
  <keyboard>
    <keymap>english-us</keymap>
  </keyboard>
  <language>
    <language>en_US</language>
    <languages/>
  </language>
  <timezone>
    <hwclock>UTC</hwclock>
    <timezone>Etc/GMT</timezone>
  </timezone>

  <!-- set up networking -->
  <networking>
    <dhcp_options>
      <dhclient_client_id/>
      <dhclient_hostname_option>AUTO</dhclient_hostname_option>
    </dhcp_options>
    <dns>
      <dhcp_hostname config:type="boolean">true</dhcp_hostname>
      <resolv_conf_policy>auto</resolv_conf_policy>
      <write_hostname config:type="boolean">false</write_hostname>
    </dns>
    <interfaces config:type="list">
      <interface>
        <bootproto>dhcp</bootproto>
        <device>eth0</device>
        <dhclient_set_default_route>yes</dhclient_set_default_route>
        <startmode>auto</startmode>
      </interface>
      <interface>
        <bootproto>static</bootproto>
        <device>lo</device>
        <firewall>no</firewall>
        <ipaddr>127.0.0.1</ipaddr>
        <netmask>255.0.0.0</netmask>
        <network>127.0.0.0</network>
        <prefixlen>8</prefixlen>
        <startmode>nfsroot</startmode>
        <usercontrol>no</usercontrol>
      </interface>
    </interfaces>
    <ipv6 config:type="boolean">true</ipv6>
    <keep_install_network config:type="boolean">true</keep_install_network>
    <setup_before_proposal config:type="boolean">true</setup_before_proposal>
    <managed config:type="boolean">false</managed>
    <routing>
      <ipv4_forward config:type="boolean">true</ipv4_forward>
      <ipv6_forward config:type="boolean">true</ipv6_forward>
    </routing>
  </networking>

  <!-- configure ntp client -->
  <ntp-client>
    <ntp_policy>auto</ntp_policy>
    <ntp_servers config:type="list">
      <ntp_server>
        <!-- replace ntp server address value bellow with one from your infrastructure -->
        <address>0.novell.pool.ntp.org</address>
        <iburst config:type="boolean">true</iburst>
        <offline config:type="boolean">true</offline>
      </ntp_server>
    </ntp_servers>
    <ntp_sync>systemd</ntp_sync>
  </ntp-client>

  <!-- install required packages -->
  <software>
    <image/>
    <products config:type="list">
      <product>SLES</product>
    </products>
    <instsource/>
    <patterns config:type="list">
      <pattern>base</pattern>
      <pattern>enhanced_base</pattern>
      <pattern>minimal_base</pattern>
      <pattern>basesystem</pattern>
    </patterns>
    <packages config:type="list">
      <package>sles-release</package>
      <package>sle-module-containers-release</package>
      <package>sle-module-basesystem-release</package>
      <package>caasp-release</package>
    </packages>
  </software>

  <services-manager>
    <default_target>multi-user</default_target>
    <services>
      <disable config:type="list">
        <service>purge-kernels</service>
      </disable>
      <enable config:type="list">
        <service>sshd</service>
        <service>chronyd</service>
      </enable>
    </services>
  </services-manager>

  <!-- disable root password and add ssh keys -->
  <users config:type="list">
    <user>
      <username>root</username>
      <user_password>linux</user_password>
      <encrypted config:type="boolean">false</encrypted>
    </user>
    <user>
      <username>sles</username>
      <user_password>linux</user_password>
      <encrypted config:type="boolean">false</encrypted>
    </user>
  </users>
<add-on>
 <add_on_products config:type="list">
  <listentry>
   <ask_on_error config:type="boolean">true</ask_on_error>
   <media_url>http://$redhat_management_server/ks/dist/child/sle-module-basesystem15-sp1-pool-x86_64/$distrotree</media_url>
   <name>sle-module-basesystem15-sp1-pool-x86_64</name>
   <product>sle-module-basesystem15-sp1-pool-x86_64</product>
   <product_dir>/</product_dir>
  </listentry>
  <listentry>
   <ask_on_error config:type="boolean">true</ask_on_error>
   <media_url>http://$redhat_management_server/ks/dist/child/sle-module-basesystem15-sp1-updates-x86_64/$distrotree</media_url>
   <name>sle-module-basesystem15-sp1-updates-x86_64</name>
   <product>sle-module-basesystem15-sp1-updates-x86_64</product>
   <product_dir>/</product_dir>
  </listentry>
  <listentry>
   <ask_on_error config:type="boolean">true</ask_on_error>
   <media_url>http://$redhat_management_server/ks/dist/child/sle-product-sles15-sp1-updates-x86_64/$distrotree</media_url>
   <name>sle-product-sles15-sp1-updates-x86_64</name>
   <product>sle-product-sles15-sp1-updates-x86_64</product>
   <product_dir>/</product_dir>
  </listentry>
  <listentry>
   <ask_on_error config:type="boolean">true</ask_on_error>
   <media_url>http://$redhat_management_server/ks/dist/child/sle-module-server-applications15-sp1-pool-x86_64/$distrotree</media_url>
   <name>sle-module-server-applications15-sp1-pool-x86_64</name>
   <product>sle-module-server-applications15-sp1-pool-x86_64</product>
   <product_dir>/</product_dir>
  </listentry>
  <listentry>
   <ask_on_error config:type="boolean">true</ask_on_error>
   <media_url>http://$redhat_management_server/ks/dist/child/sle-module-server-applications15-sp1-updates-x86_64/$distrotree</media_url>
   <name>sle-module-server-applications15-sp1-updates-x86_64</name>
   <product>sle-module-server-applications15-sp1-updates-x86_64</product>
   <product_dir>/</product_dir>
  </listentry>
  <listentry>
    <media_url>http://$redhat_management_server/ks/dist/child/sle-manager-tools15-pool-x86_64-sp1/$distrotree</media_url>
    <name>sle-manager-tools15-pool-x86_64-sp1</name>
   <product>sle-manager-tools15-pool-x86_64-sp1</product>
   <product_dir>/</product_dir>
  </listentry>
  <listentry>
    <media_url>http://$redhat_management_server/ks/dist/child/sle-manager-tools15-pool-x86_64-sp1/$distrotree</media_url>
    <name>sle-manager-tools15-updates-x86_64-sp1</name>
   <product>sle-manager-tools15-updates-x86_64-sp1</product>
   <product_dir>/</product_dir>
  </listentry>
    <listentry>
    <media_url>http://$redhat_management_server/ks/dist/child/sle-module-containers15-sp1-pool-x86_64/$distrotree</media_url>
    <name>sle-module-containers15-sp1-pool-x86_64 </name>
   <product>sle-module-containers15-sp1-pool-x86_64</product>
   <product_dir>/</product_dir>
  </listentry>
    <listentry>
    <media_url>http://$redhat_management_server/ks/dist/child/sle-module-containers15-sp1-updates-x86_64/$distrotree</media_url>
    <name>sle-module-containers15-sp1-updates-x86_64</name>
   <product>sle-module-containers15-sp1-updates-x86_64</product>
   <product_dir>/</product_dir>
  </listentry>
    <listentry>
    <media_url>http://$redhat_management_server/ks/dist/child/suse-caasp-4.0-pool-x86_64-sp1/$distrotree</media_url>
    <name>suse-caasp-4.0-pool-x86_64-sp1</name>
   <product>suse-caasp-4.0-pool-x86_64-sp1</product>
   <product_dir>/</product_dir>
  </listentry>
    <listentry>
    <media_url>http://$redhat_management_server/ks/dist/child/suse-caasp-4.0-updates-x86_64-sp1/$distrotree</media_url>
    <name>suse-caasp-4.0-updates-x86_64-sp1</name>
   <product>suse-caasp-4.0-updates-x86_64-sp1</product>
   <product_dir>/</product_dir>
  </listentry>
 </add_on_products>
</add-on>
 <!-- register -->
  <suse_register>
    <do_registration config:type="boolean">true</do_registration>
    <install_updates config:type="boolean">true</install_updates>
    <email><!-- replace this comment with an email address used for registration --></email>
    <reg_code><!-- replace this comment with a CaaSP registration code --></reg_code>
    <slp_discovery config:type="boolean">false</slp_discovery>
    <addons config:type="list">
      <addon>
        <name>sle-module-containers</name>
        <version>15.1</version>
        <arch>x86_64</arch>
      </addon>
      <addon>
        <name>caasp</name>
        <version>4.0</version>
        <arch>x86_64</arch>
        <reg_code><!-- replace this comment with a CaaSP registration code --></reg_code>
      </addon>
    </addons>
  </suse_register>

  <scripts>
  <chroot-scripts config:type="list">
      <script>
        <chrooted config:type="boolean">true</chrooted>
        <filename>add_sles_sudo_rule.sh</filename>
        <interpreter>shell</interpreter>
        <source>
<![CDATA[
#!/bin/sh
echo "Defaults:sles !targetpw
sles ALL=(ALL,ALL) NOPASSWD: ALL" > /etc/sudoers.d/sles
]]>
          </source>
      </script>
    </chroot-scripts>
<init-scripts config:type="list">
      $SNIPPET('spacewalk/minion_script')
    </init-scripts>
</scripts>
</profile>
----

== Manage a {caasp} Cluster With {productname}

You can use {productname} to manage one or more existing {caasp} clusters.

[NOTE]
====
Only {caasp}{nbsp}4 is currently supported.
====


Before you begin, ensure you have installed your {caasp} cluster.
You will need to manually copy some configuration information from your cluster to the {productname} Server:

* Copy the ``skuba`` configuration directory from your cluster to the {productname} Server, and take a note of the file location.
// Default file location? --LKB 2020-06-04
This is the directory that the ``skuba`` service creates after the cluster has been bootstrapped.
* Copy the passwordless private SSH key used to access the cluster nodes to the {productname} Server, and take a note of the file location.
You need the current keys, and keys for any clients that you want to use in the future.
Alternatively, you can use an ``ssh-agent`` socket, and provide the path to the socket when setting up the cluster.



=== Elect a Management Node

To manage a {caasp} cluster, you need to elect a client as the management node for the cluster.
The management node cannot be part of the cluster, and it must have the {caasp} channels associated with it before you begin.
You can use a single management node for multiple clusters, as long as the clusters are all of the same kind.



.Procedure: Electing a Management Node
. In the {productname} {webui}, navigate to menu:Systems[System List] and click the name of the client to elect as the management node.
. Navigate to the menu:Formulas[Configuration] tab, and check the ``CaaSP Management Node`` formula.
. Click btn:[Save] and apply the highstate.


[NOTE]
====
You will not be able to use the management node until the highstate has been completed.
====


List all known clusters by navigating to menu:Clusters[Overview].
This list displays all existing clusters, along with the cluster type, and which management node they are associated with.
It also shows the nodes within the cluster, if the nodes are registered to {productname}.
For the nodes within a cluster, additional information from ``skuba`` and the {k8s} API are shown, including the role, status, and whether any updates are available.

For more information about the data available for nodes, see https://documentation.suse.com/suse-caasp/4/html/caasp-admin/_cluster_updates.html.



=== Manage Clusters

To manage a cluster in {productname}, add the cluster in the {webui}.



.Procedure: Adding an Existing Cluster
. In the {productname} {webui}, navigate to menu:Clusters[Overview] and click btn:[FIXME].
. Follow the prompts to provide information about your cluster, including the cluster type, and select the management node to associate.
. Type the path to the ``skuba`` configuration file for the cluster.
// For example?
. Type the passwordless SSH key you want to use, or to the ``ssh-agent`` socket.
. Type a name, label, and description for the cluster.
. Click btn:[FIXME].


For each cluster you manage with {productname}, a corresponding system group is created.
By default, the system group is called ``Cluster <cluster_name>``.
Refresh the system group to update the list of nodes.
Only nodes known to {productname} are shown.


You can remove clusters from {productname} by navigating to menu:Clusters[Overview], unchecking the cluster to be deleted, and clicking btn:[Delete Cluster].


[IMPORTANT]
====
Deleting a cluster removes the cluster from {productname}, it does not delete the cluster nodes.
Workloads running on the cluster will continue uninterrupted.
====



=== Manage Nodes

When you have the cluster created in {productname}, you can manage nodes within the cluster.

Before you add a new node to the cluster, check the management node can access the node you want to add using passwordless SSH, or the ``ssh-agent`` socket you are forwarding.

You also need to ensure that the node you want to add is registered to {productname}, and has a {caasp} channel assigned.


.Procedure: Adding a Node to a Cluster
. In the {productname} {webui}, navigate to menu:Clusters[Overview] and click btn:[Join Node].
. Select the node to add from a list of available nodes.
The list of available nodes includes only nodes that are registered to {productname}, are not management nodes, and are not currently part of any cluster.
. Follow the prompts to enter the {caasp} parameters for the node to be added.
. OPTIONAL: Specify a custom ``ssh-agent`` socket that is valid only for the nodes that are being added.
. Click btn:[Save] to schedule an action to add the node.
During this action, {productname} prepares the node for joining by disabling swap, then joins the node to the cluster.



.Procedure: Removing a Node from a Cluster
. In the {productname} {webui}, navigate to menu:Clusters[Overview], uncheck the node to remove, and click btn:[Remove Node].
. Follow the prompts to define the parameters for the node to be removed.
. OPTIONAL: Specify a custom ``ssh-agent`` socket that is valid only for the node that is being removed.
. Click btn:[Save] to schedule an action to remove the node.

For more information about node removal, see https://documentation.suse.com/suse-caasp/4/single-html/caasp-admin/#_permanent_removal.



==== Upgrade the Cluster

If the cluster has available updates, you can use {productname} to schedule and manage the upgrade.

{productname} upgrades all control planes first, and then upgrades the workers.
For more information, see https://documentation.suse.com/suse-caasp/4.2/single-html/caasp-admin/#_cluster_updates.


.Procedure: Upgrading the Cluster
. In the {productname} {webui}, navigate to menu:Clusters[Overview], and click the cluster to upgrade.
. OPTIONAL: The are no {caasp} parameters available for you to customize for upgrade.
However, you can specify a custom ``ssh-agent`` socket that is valid only for the nodes that are being upgraded.
. Click btn:[Save] to schedule an action to upgrade the cluster.


[NOTE]
====
{productname} will only interact with ``skuba`` to upgrade the cluster.
Any other required action, such as configuration changes, are not issued by {productname}.
====


For more information about upgrading, see https://www.suse.com/releasenotes/x86_64/SUSE-CAASP/4.
